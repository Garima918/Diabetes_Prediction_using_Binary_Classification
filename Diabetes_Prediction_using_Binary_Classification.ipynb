{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Garima918/Diabetes-Prediction-based-on-Binary-Classification/blob/main/Diabetes_Prediction_using_Binary_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Loading**"
      ],
      "metadata": {
        "id": "1OLiUmnJUi3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JObaWuzl8W3L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#If we want to change working directory. Below are the steps:\n",
        "#import os\n",
        "#os.chdir()\n",
        "#print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Required Libraries**"
      ],
      "metadata": {
        "id": "-RIhLwRlKJJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T-mHPmi8Z_8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Diabetes_dataset.csv')\n",
        "# We can import file via local repository/drive or through Dataset libraries like Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Describing Dataset**"
      ],
      "metadata": {
        "id": "Ze_IkHPnU_Fe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQdeFrbt8dgo"
      },
      "outputs": [],
      "source": [
        "print(df.info())\n",
        "print(df.describe())\n",
        "# If we get total entries equal to the entries in all variable types, then we can conclude that there are no missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Visualization**"
      ],
      "metadata": {
        "id": "0os_4pudUvwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TQUZyEU8dpw"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.histplot([df['Glucose'], df['Age'], df['BMI']], bins= 50, kde=True, alpha=0.8, palette = ['orange','blue','green'])\n",
        "# We can add as many columns of dataframe to visualize and change other paramenters as required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrWw0c9w8dsu"
      },
      "outputs": [],
      "source": [
        "k = df.corr()\n",
        "sns.heatmap(k, annot=True, cmap='Blues')\n",
        "#Other colormaps can be - bwr, reds, oranges etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52hAtHQ98dvj"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df, hue='Outcome')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Splitting**"
      ],
      "metadata": {
        "id": "k-i_kLBgVrTd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB7AeUpI8dx1"
      },
      "outputs": [],
      "source": [
        "x = df.drop(columns = 'Outcome',axis=1)\n",
        "y = df['Outcome']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Flt5k3lZnW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 9)\n",
        "pmf = pd.DataFrame()\n",
        "#pmf- performance_metrics_file\n",
        "#Additional dataframe (pmf) created to store all the model's performance score for better visualization among models trained."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "c5j7y4avWc4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Class Imbalance**"
      ],
      "metadata": {
        "id": "4hD7L0OyWjhz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmgxA11_8d4h"
      },
      "outputs": [],
      "source": [
        "#from imblearn.over_sampling import SMOTE\n",
        "#sm = SMOTE(random_state=9)\n",
        "#X, Y = sm.fit_resample(x, y)\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=9)\n",
        "x, y = ros.fit_resample(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Scaling**"
      ],
      "metadata": {
        "id": "_5z7AJOxXkY8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSqcwfnK8d6q"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "data_scaler = StandardScaler()\n",
        "data_rescaled = data_scaler.fit_transform(x)\n",
        "x = pd.DataFrame(data_rescaled, columns = x.columns)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance Measures**\n"
      ],
      "metadata": {
        "id": "0FzBjrQhDUuj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzajp3EM8d9K"
      },
      "outputs": [],
      "source": [
        "#Importing modules for Performance Metrics\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_indicator = ['Precision','Accuracy','Recall','F1 Score', 'ROC AUC']\n",
        "#For more information, please refer to README.md on GitHub for better explanation of this particular variable in this dataset."
      ],
      "metadata": {
        "id": "OBzugfhsqwWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Logistic Regression Model**"
      ],
      "metadata": {
        "id": "f613ZeEPZFeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJtZH_mZ8d-8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "x_train, x_test, y_train, y_test = x_train.copy(), x_test.copy(), y_train.copy(), y_test.copy()\n",
        "log_r = LogisticRegression()\n",
        "log_r.fit(x_train, y_train)\n",
        "log_r_pred = log_r.predict(x_test)\n",
        "\n",
        "log_r_precision_score = precision_score(y_test, log_r_pred)\n",
        "log_r_accuracy_score = accuracy_score(y_test, log_r_pred)\n",
        "log_r_recall_score = recall_score(y_test, log_r_pred)\n",
        "log_r_f1_score = f1_score(y_test, log_r_pred)\n",
        "log_r_fpr, log_r_tpr, log_r_thresholds = roc_curve(y_test, log_r_pred)\n",
        "log_r_roc_auc = auc(log_r_fpr, log_r_tpr)\n",
        "\n",
        "pmf[\"Log_R\"] = pd.DataFrame({'Log R': [log_r_precision_score, log_r_accuracy_score, log_r_recall_score, log_r_f1_score, log_r_roc_auc]}, index = metrics_indicator)\n",
        "cm = confusion_matrix(y_test, log_r_pred, labels = log_r.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = log_r.classes_)\n",
        "disp.plot()\n",
        "plt.title('Logistic Regression')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Decision Tree Model**"
      ],
      "metadata": {
        "id": "3V209KlTorsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFj7dE428eBV"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(x_train, y_train)\n",
        "dt_pred = dt.predict(x_test)\n",
        "\n",
        "dt_precision_score = precision_score(y_test, dt_pred)\n",
        "dt_accuracy_score = accuracy_score(y_test, dt_pred)\n",
        "dt_recall_score = recall_score(y_test, dt_pred)\n",
        "dt_f1_score = f1_score(y_test, dt_pred)\n",
        "dt_fpr, dt_tpr, dt_thresholds = roc_curve(y_test, dt_pred)\n",
        "dt_roc_auc = auc(dt_fpr, dt_tpr)\n",
        "\n",
        "pmf[\"DT\"] = pd.DataFrame({'DT':[dt_precision_score, dt_accuracy_score, dt_recall_score, dt_f1_score, dt_roc_auc]}, index = metrics_indicator)\n",
        "cm1 = confusion_matrix(y_test, dt_pred,labels = dt.classes_)\n",
        "disp1 = ConfusionMatrixDisplay(confusion_matrix = cm1, display_labels = dt.classes_)\n",
        "disp1.plot()\n",
        "plt.title('Decision Tree')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Random Forest Classifier**"
      ],
      "metadata": {
        "id": "jo26jxpqsNDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(x_train,y_train)\n",
        "rfc_pred = rfc.predict(x_test)\n",
        "\n",
        "rfc_precision_score = precision_score(y_test, rfc_pred)\n",
        "rfc_accuracy_score = accuracy_score(y_test, rfc_pred)\n",
        "rfc_recall_score = recall_score(y_test, rfc_pred)\n",
        "rfc_f1_score = f1_score(y_test, rfc_pred)\n",
        "rfc_fpr, rfc_tpr, rfc_thresholds = roc_curve(y_test, rfc_pred)\n",
        "rfc_roc_auc = auc(rfc_fpr, rfc_tpr)\n",
        "\n",
        "pmf[\"RFC\"] = pd.DataFrame({\"RFC\":[rfc_precision_score, rfc_accuracy_score,rfc_recall_score, rfc_f1_score, rfc_roc_auc]}, index = metrics_indicator)\n",
        "\n",
        "cm2 = confusion_matrix(y_test, rfc_pred, labels = rfc.classes_)\n",
        "disp2 = ConfusionMatrixDisplay(cm2, display_labels = rfc.classes_)\n",
        "disp2.plot()\n",
        "plt.title('Random Forest Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wxs5BaJTsRLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting KNN Model**"
      ],
      "metadata": {
        "id": "zCuLJRNWts96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdnOBRAi8eDx"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(x_train,y_train)\n",
        "knn_pred = knn.predict(x_test)\n",
        "\n",
        "knn_precision_score = precision_score(y_test, knn_pred)\n",
        "knn_accuracy_score = accuracy_score(y_test, knn_pred)\n",
        "knn_recall_score = recall_score(y_test, knn_pred)\n",
        "knn_f1_score = f1_score(y_test, knn_pred)\n",
        "knn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, knn_pred)\n",
        "knn_roc_auc = auc(knn_fpr, knn_tpr)\n",
        "\n",
        "pmf[\"KNN\"] = pd.DataFrame({\"KNN\":[knn_precision_score, knn_accuracy_score, knn_recall_score, knn_f1_score, knn_roc_auc]}, index = metrics_indicator)\n",
        "\n",
        "cm3 = confusion_matrix(y_test, knn_pred, labels = knn.classes_)\n",
        "disp = ConfusionMatrixDisplay(cm3, display_labels = knn.classes_)\n",
        "disp.plot()\n",
        "plt.title('KNN')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Naive Bayes Model**"
      ],
      "metadata": {
        "id": "h9p9eIxQobYB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alUQ0Vid8eGB"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "nb = GaussianNB()\n",
        "nb.fit(x_train, y_train)\n",
        "nb_pred = nb.predict(x_test)\n",
        "\n",
        "nb_precision_score = precision_score(y_test, nb_pred)\n",
        "nb_accuracy_score = accuracy_score(y_test, nb_pred)\n",
        "nb_recall_score = recall_score(y_test, nb_pred)\n",
        "nb_f1_score = f1_score(y_test, nb_pred)\n",
        "nb_fpr, nb_tpr, nb_thresholds = roc_curve(y_test, nb_pred)\n",
        "nb_roc_auc = auc(nb_fpr, nb_tpr)\n",
        "\n",
        "pmf[\"NB\"] = pd.DataFrame({\"NB\": [nb_precision_score, nb_accuracy_score, nb_recall_score, nb_f1_score, nb_roc_auc]}, index = metrics_indicator)\n",
        "\n",
        "cm4 = confusion_matrix(y_test, nb_pred, labels = nb.classes_)\n",
        "disp = ConfusionMatrixDisplay(cm4, display_labels = nb.classes_)\n",
        "disp.plot()\n",
        "plt.title(\"Naive Bayes\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Gradient Boosting Model**"
      ],
      "metadata": {
        "id": "gaSXGGZSqZ7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS9rI_yr8eIU"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(x_train,y_train)\n",
        "gb_pred = gb.predict(x_test)\n",
        "\n",
        "gb_precision_score = precision_score(y_test, gb_pred)\n",
        "gb_accuracy_score = accuracy_score(y_test, gb_pred)\n",
        "gb_recall_score = recall_score(y_test, gb_pred)\n",
        "gb_f1_score = f1_score(y_test, gb_pred)\n",
        "gb_fpr, gb_tpr, gb_thresholds = roc_curve(y_test, gb_pred)\n",
        "gb_roc_auc = auc(gb_fpr, gb_tpr)\n",
        "\n",
        "pmf[\"GB\"] = pd.DataFrame({\"GB\":[gb_precision_score, gb_accuracy_score, gb_recall_score, gb_f1_score, gb_roc_auc]}, index = metrics_indicator)\n",
        "\n",
        "cm5 = confusion_matrix(y_test, gb_pred, labels = gb.classes_)\n",
        "disp= ConfusionMatrixDisplay(cm5, display_labels = gb.classes_)\n",
        "disp.plot()\n",
        "plt.title(\"Gradient Boosting\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting Ada Boost Model**"
      ],
      "metadata": {
        "id": "GZqf4OTusYCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ3yuMYt8eKt"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "x_train, y_train, x_test, y_test = x_train.copy(), y_train.copy(), x_test.copy(), y_test.copy()\n",
        "ab = AdaBoostClassifier()\n",
        "ab.fit(x_train, y_train)\n",
        "ab_pred = ab.predict(x_test)\n",
        "\n",
        "ab_precision_score = precision_score(y_test, ab_pred)\n",
        "ab_accuracy_score = accuracy_score(y_test, ab_pred)\n",
        "ab_recall_score = recall_score(y_test, ab_pred)\n",
        "ab_f1_score = f1_score(y_test, ab_pred)\n",
        "ab_fpr, ab_tpr, ab_thresholds = roc_curve(y_test, ab_pred)\n",
        "ab_roc_auc = auc(ab_fpr, ab_tpr)\n",
        "\n",
        "pmf[\"AB\"] = pd.DataFrame({\"AB\":[ab_precision_score, ab_accuracy_score, ab_recall_score, ab_f1_score, ab_roc_auc]}, index = metrics_indicator)\n",
        "\n",
        "cm6 = confusion_matrix(y_test, ab_pred, labels = ab.classes_)\n",
        "disp= ConfusionMatrixDisplay(cm6, display_labels = ab.classes_)\n",
        "disp.plot()\n",
        "plt.title(\"Ada Boost\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Parameter to Display ROC Curve**"
      ],
      "metadata": {
        "id": "0PFdiHhUubH2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCLT2R6c8eNJ"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "#plt.plot(lr_fpr, lr_tpr, color = 'darkorange', lw = 2, label = 'Linear Regression(area=%0.3f)' % lr_roc_auc)\n",
        "plt.plot(log_r_fpr, log_r_tpr, color = 'yellow', lw = 2, label = 'Logistic Regression (area = %0.3f)' % log_r_roc_auc)\n",
        "plt.plot(dt_fpr, dt_tpr, color = 'red', lw = 2, label = 'Decision Tree (area = %0.3f)' % dt_roc_auc)\n",
        "plt.plot(rfc_fpr, rfc_tpr, color = 'green', lw =2, label = 'Random Forest (area = %0.3f)' % rfc_roc_auc)\n",
        "plt.plot(knn_fpr, knn_tpr, color = 'cyan', lw = 2, label = 'KNN (area = %0.3f)' % knn_roc_auc)\n",
        "plt.plot(nb_fpr, nb_tpr, color = 'blue', lw = 2, label = 'Naive Bayes (area = %0.3f)' % nb_roc_auc)\n",
        "plt.plot(gb_fpr, gb_tpr, color = 'black', lw = 2, label = 'Gradient Boosting (area = %0.3f)' % gb_roc_auc)\n",
        "plt.plot(ab_fpr, ab_tpr, color = 'grey', lw = 2, label = 'Ada Boost (area = %0.3f)' % ab_roc_auc)\n",
        "\n",
        "plt.legend(loc=0)\n",
        "plt.xlim([0.0,1.0])\n",
        "plt.ylim([0.0,1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.plot([0,1], [0,1], linestyle = '--', color = 'black', lw = 2, label = 'Random Classifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Writing Performance Metrics to File**"
      ],
      "metadata": {
        "id": "VflNVchgEvsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pmf_ad = pd.DataFrame({[pmf.keys]:[pmf.values]}, index = metrics_indicator)\n",
        "pmf.to_csv(\"./Performance_Of_Models_After_Preprocessing.csv\")"
      ],
      "metadata": {
        "id": "yQ2tAIBKD_Jc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1m6cp00LqyrMphUpjXQ5jDXmHRGmbbz_4",
      "authorship_tag": "ABX9TyMlzU2pnlwEOhxwkBuM8UOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}